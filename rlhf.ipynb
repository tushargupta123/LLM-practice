{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning from human feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting trl\n",
      "  Obtaining dependency information for trl from https://files.pythonhosted.org/packages/ce/b2/b0e9c7a15d666aebe83ed72b6a3bec869be88246ddf22d8953f3eee61e22/trl-0.7.2-py3-none-any.whl.metadata\n",
      "  Downloading trl-0.7.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from trl) (2.1.0)\n",
      "Requirement already satisfied: transformers>=4.18.0 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from trl) (4.32.1)\n",
      "Requirement already satisfied: numpy>=1.18.2 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from trl) (1.24.3)\n",
      "Requirement already satisfied: accelerate in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from trl) (0.24.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from trl) (2.12.0)\n",
      "Collecting tyro>=0.5.7 (from trl)\n",
      "  Obtaining dependency information for tyro>=0.5.7 from https://files.pythonhosted.org/packages/f0/ee/72cb2647dc72ef6412e5cbca8bbbae7f757c872ca047e421a2d78d7b890f/tyro-0.5.12-py3-none-any.whl.metadata\n",
      "  Downloading tyro-0.5.12-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from torch>=1.4.0->trl) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from torch>=1.4.0->trl) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from torch>=1.4.0->trl) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from torch>=1.4.0->trl) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from torch>=1.4.0->trl) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from torch>=1.4.0->trl) (2023.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from transformers>=4.18.0->trl) (0.15.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\tusha\\appdata\\roaming\\python\\python311\\site-packages (from transformers>=4.18.0->trl) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from transformers>=4.18.0->trl) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from transformers>=4.18.0->trl) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from transformers>=4.18.0->trl) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from transformers>=4.18.0->trl) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from transformers>=4.18.0->trl) (0.3.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from transformers>=4.18.0->trl) (4.65.0)\n",
      "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.7->trl)\n",
      "  Downloading docstring_parser-0.15-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from tyro>=0.5.7->trl) (13.5.2)\n",
      "Collecting shtab>=1.5.6 (from tyro>=0.5.7->trl)\n",
      "  Obtaining dependency information for shtab>=1.5.6 from https://files.pythonhosted.org/packages/86/69/3a4873b36d65a1b8f4ee606f5a785b5babb9960385802de60d8455e2f8b6/shtab-1.6.4-py3-none-any.whl.metadata\n",
      "  Downloading shtab-1.6.4-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: colorama>=0.4.0 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from tyro>=0.5.7->trl) (0.4.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\tusha\\appdata\\roaming\\python\\python311\\site-packages (from accelerate->trl) (5.9.5)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from datasets->trl) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from datasets->trl) (0.3.6)\n",
      "Requirement already satisfied: pandas in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from datasets->trl) (2.0.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from datasets->trl) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from datasets->trl) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from datasets->trl) (3.8.3)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from datasets->trl) (0.13.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from aiohttp->datasets->trl) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from aiohttp->datasets->trl) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from aiohttp->datasets->trl) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from aiohttp->datasets->trl) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from aiohttp->datasets->trl) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from aiohttp->datasets->trl) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from aiohttp->datasets->trl) (1.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from requests->transformers>=4.18.0->trl) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from requests->transformers>=4.18.0->trl) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from requests->transformers>=4.18.0->trl) (2023.7.22)\n",
      "Requirement already satisfied: six in c:\\users\\tusha\\appdata\\roaming\\python\\python311\\site-packages (from responses<0.19->datasets->trl) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.7->trl) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\tusha\\appdata\\roaming\\python\\python311\\site-packages (from rich>=11.1.0->tyro>=0.5.7->trl) (2.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.4.0->trl) (2.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tusha\\appdata\\roaming\\python\\python311\\site-packages (from pandas->datasets->trl) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from pandas->datasets->trl) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from pandas->datasets->trl) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\tusha\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.7->trl) (0.1.0)\n",
      "Downloading trl-0.7.2-py3-none-any.whl (124 kB)\n",
      "   ---------------------------------------- 0.0/124.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 124.0/124.0 kB 2.4 MB/s eta 0:00:00\n",
      "Downloading tyro-0.5.12-py3-none-any.whl (99 kB)\n",
      "   ---------------------------------------- 0.0/99.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 99.9/99.9 kB 2.9 MB/s eta 0:00:00\n",
      "Downloading shtab-1.6.4-py3-none-any.whl (13 kB)\n",
      "Installing collected packages: shtab, docstring-parser, tyro, trl\n",
      "Successfully installed docstring-parser-0.15 shtab-1.6.4 trl-0.7.2 tyro-0.5.12\n"
     ]
    }
   ],
   "source": [
    "!pip install trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline,AutoTokenizer,AutoModelForSequenceClassification,AutoModelForSeq2SeqLM,GenerationConfig\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel,PeftConfig,LoraConfig,TaskType\n",
    "\n",
    "from trl import PPOTrainer,PPOConfig,AutoModelForSeq2SeqLMWithValueHead,create_reference_model\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load FLAN-T5 Model , Prepare Reward Model and Toxicity Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/tusha/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4225d464dfb749e1bb8c5967c6b9cb38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'google/flan-t5-base'\n",
    "huggingface_dataset_name = 'knkarthick/dialogsum'\n",
    "\n",
    "dataset_original = load_dataset(huggingface_dataset_name)\n",
    "dataset_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (C:/Users/tusha/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-cd36827d3490488d/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n",
      "Loading cached processed dataset at C:\\Users\\tusha\\.cache\\huggingface\\datasets\\knkarthick___csv\\knkarthick--dialogsum-cd36827d3490488d\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-d904df43a047dc8b.arrow\n",
      "Loading cached processed dataset at C:\\Users\\tusha\\.cache\\huggingface\\datasets\\knkarthick___csv\\knkarthick--dialogsum-cd36827d3490488d\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-722906dcf7d23a9b.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
      "        num_rows: 8017\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'input_ids', 'query'],\n",
      "        num_rows: 2005\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(model_name,dataset_name,input_min_text_length,input_max_text_length):\n",
    "    dataset = load_dataset(dataset_name,split=\"train\")\n",
    "    dataset = dataset.filter(lambda x: len(x['dialogue']) > input_min_text_length and len(x['dialogue']) <= input_max_text_length, batched=False)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name,device_map=\"auto\")\n",
    "    def tokenize(sample):\n",
    "        prompt = f\"\"\"\n",
    "Summarize the following conversation. \n",
    "{sample['dialogue']}\n",
    "\n",
    "summary:\n",
    "\"\"\"\n",
    "        sample['input_ids'] = tokenizer.encode(prompt)\n",
    "        sample['query'] = tokenizer.decode(sample['input_ids'])\n",
    "        return sample\n",
    "    dataset = dataset.map(tokenize,batched=False)\n",
    "    dataset.set_format(type='torch')\n",
    "\n",
    "    dataset_splits = dataset.train_test_split(test_size=0.2,shuffle=False,seed=42)\n",
    "    return dataset_splits\n",
    "\n",
    "dataset = build_dataset(model_name = model_name,dataset_name= huggingface_dataset_name,input_min_text_length=200,input_max_text_length=1000)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _,param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params+=param.numel()\n",
    "    return f\" trainable model parameters: {trainable_model_params}\\n all model parameters: {all_model_params}\\n percentage of trainable model: {trainable_model_params*100/all_model_params}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    target_modules=['q','v'],\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name,torch_dtype=torch.bfloat16)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model,\n",
    "                                       './peft-dialogue-summary-checkpoint-from-s3/',\n",
    "                                       lora_config=lora_config,\n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       device_map=\"auto\",\n",
    "                                       is_trainable=True)\n",
    "print(f'PEFT model parameters to be updated:\\n{print_number_of_trainable_model_parameters(peft_model)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,torch_dtype=torch.bfloat16,is_trainable=True)\n",
    "print(f'PPO model parameters to be updated (VlaueHead + 769 params) : \\n {print_number_of_trainable_model_parameters(ppo_model)}')\n",
    "print(ppo_model.v_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_model = create_reference_model(ppo_model)\n",
    "print(f'Reference model parameters to be updated :\\n{print_number_of_trainable_model_parameters(ref_model)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Reward model\n",
    "we will use Facebook's RoBERTa-based hate speech model for the reward model. this model will output logits and then predict probabilities across two class :nothate and hate. the logits of the output nothate will be taken as a postive reward. then model will be fine tuned with PPO using those reqard values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'nothate', 1: 'hate'}\n"
     ]
    }
   ],
   "source": [
    "toxicity_model_name = 'facebook/roberta-hate-speech-dynabench-r4-target'\n",
    "toxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name,device_map=\"auto\")\n",
    "toxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name,device_map=\"auto\")\n",
    "print(toxicity_model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [not hate,hate]: [2.307953357696533, -2.0024046897888184]\n",
      "probabilities [not hate,hate]: [0.9867492318153381, 0.013250798918306828]\n",
      "reward (high): [2.307953357696533]\n"
     ]
    }
   ],
   "source": [
    "non_toxic_text = 'i want to kiss you'\n",
    "toxicity_input_ids = toxicity_tokenizer(non_toxic_text,return_tensors='pt').input_ids\n",
    "logits = toxicity_model(input_ids = toxicity_input_ids).logits\n",
    "print(f'logits [not hate,hate]: {logits.tolist()[0]}')\n",
    "probabilities = logits.softmax(dim=1).tolist()[0]\n",
    "print(f'probabilities [not hate,hate]: {probabilities}')\n",
    "not_hate_index = 0\n",
    "nothate_reward = (logits[:,not_hate_index]).tolist()\n",
    "print(f'reward (high): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward model output for non-toxic text:\n",
      "[{'label': 'nothate', 'score': 2.307953357696533}, {'label': 'hate', 'score': -2.0024046897888184}]\n",
      "[{'label': 'nothate', 'score': 0.9867492318153381}, {'label': 'hate', 'score': 0.013250799849629402}]\n"
     ]
    }
   ],
   "source": [
    "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\",\n",
    "                          model=toxicity_model_name,\n",
    "                          device=device)\n",
    "\n",
    "reward_logits_kwargs = {\n",
    "    \"top_k\":None,\n",
    "    \"function_to_apply\":\"none\",\n",
    "    \"batch_size\":16\n",
    "}\n",
    "\n",
    "reward_probabilities_kwargs = {\n",
    "    \"top_k\":None,\n",
    "    \"function_to_apply\":\"softmax\",\n",
    "    \"batch_size\":16\n",
    "}\n",
    "\n",
    "print(\"Reward model output for non-toxic text:\")\n",
    "print(sentiment_pipe(non_toxic_text,**reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_text,**reward_probabilities_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluating toxicity score (0-1)\n",
    "1 means more toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxicity_evaluator = evaluate.load(\"toxicity\",\n",
    "                                   toxicity_model_name,\n",
    "                                   module_type=\"measurement\",\n",
    "                                   toxic_label=\"hate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity score for non-toxic text:  [0.013250799849629402]\n"
     ]
    }
   ],
   "source": [
    "toxicity_score = toxicity_evaluator.compute(predictions=[non_toxic_text])\n",
    "print(\"Toxicity score for non-toxic text: \",toxicity_score['toxicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_toxicity(model,toxicity_evaluator,tokenizer,dataset,num_samples):\n",
    "    max_new_tokens = 100\n",
    "    toxicities = []\n",
    "    input_texts = []\n",
    "    for i, sample in tqdm(enumerate(dataset)):\n",
    "        input_text = sample['query']\n",
    "        if i > num_samples:\n",
    "            break\n",
    "        input_ids = tokenizer(input_text,return_tensors='pt',padding=True).input_ids\n",
    "        generation_config = GenerationConfig(max_new_tokens=max_new_tokens,\n",
    "                                             top_k=0.0,\n",
    "                                             top_p=1.0,\n",
    "                                             do_sample=True)\n",
    "        response_token_ids = model.generate(input_ids=input_ids,generation_config=generation_config)\n",
    "        generated_text = tokenizer.decode(response_token_ids[0],skip_special_tokens=True)\n",
    "        toxicity_score=toxicity_evaluator.compute(predictions=[(input_text+\" \"+generated_text)])\n",
    "        toxicities.extend(toxicity_score['toxicity'])\n",
    "\n",
    "    mean = np.mean(toxicities)\n",
    "    std = np.std(toxicities)\n",
    "    return mean,std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name,device_map=\"auto\")\n",
    "mean_before_detoxification,std_before_detoxification = evaluate_toxicity(model=ref_model,\n",
    "                                                                         toxicity_evaluator=toxicity_evaluator,\n",
    "                                                                         tokenizer=tokenizer,\n",
    "                                                                         dataset=dataset['test'],\n",
    "                                                                         num_sample=10)\n",
    "print(f'toxicity [mean,std] before detox: [{mean_before_detoxification},{std_before_detoxification}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform fine tuning to detoxify the summaries\n",
    "optimize a RL policy against the reward model using Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collator input: [{'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}]\n",
      "Collator output: {'key1': ['value1'], 'key2': ['value2'], 'key3': ['value3']}\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1.41e-5\n",
    "max_ppo_epochs = 1\n",
    "mini_batch_size = 4\n",
    "batch_size=16\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=model_name,\n",
    "    learning_rate=learning_rate,\n",
    "    ppo_epochs=max_ppo_epochs,\n",
    "    mini_batch_size=mini_batch_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key,[d[key] for d in data]) for key in data[0])\n",
    "test_data = [{\"key1\": \"value1\", \"key2\": \"value2\", \"key3\": \"value3\"}]\n",
    "print(f'Collator input: {test_data}')\n",
    "print(f'Collator output: {collator(test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_trainer = PPOTrainer(config=config, \n",
    "                         model=ppo_model, \n",
    "                         ref_model=ref_model, \n",
    "                         tokenizer=tokenizer, \n",
    "                         dataset=dataset[\"train\"], \n",
    "                         data_collator=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_length_sampler = LengthSampler(100,400)\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\":5,\n",
    "    \"top_k\":0.0,\n",
    "    \"top_p\":1.0,\n",
    "    \"so_sample\":True\n",
    "}\n",
    "\n",
    "reward_kwargs = {\n",
    "    \"top_k\":None,\n",
    "    \"function_to_apply\":\"none\",\n",
    "    \"batch_size\":16\n",
    "}\n",
    "\n",
    "max_ppo_steps = 10\n",
    "\n",
    "for step,batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    if step >= max_ppo_steps:\n",
    "        break\n",
    "    prompt_tensors = batch['input_ids']\n",
    "    summary_tensors = []\n",
    "    for prompt_tensor in prompt_tensors:\n",
    "        max_new_tokens = output_length_sampler()\n",
    "\n",
    "        generation_kwargs['max_new_tokens'] = max_new_tokens\n",
    "        summary = ppo_trainer.generate(prompt_tensor, **generation_kwargs)\n",
    "        summary_tensors.append(summary.squeeze()[-max_new_tokens:])\n",
    "\n",
    "    batch['response'] = [tokenizer.decode(r.squeeze()) for r in summary_tensors]\n",
    "\n",
    "    query_response_pairs = [q+r for q,r in zip(batch['query',batch['response']])]\n",
    "    rewards = sentiment_pipe(query_response_pairs,**reward_kwargs)\n",
    "\n",
    "    reward_tensors = [torch.tensor(reward[not_hate_index]['score']) for reward in rewards]\n",
    "\n",
    "    stats = ppo_trainer.step(prompt_tensors,summary_tensors,reward_tensors)\n",
    "    ppo_trainer.log_stats(stats,batch,reward_tensors)\n",
    "\n",
    "    print(f'objective/kl: {stats[\"objective/kl\"]}')\n",
    "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
    "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
    "    print('-'.join('' for x in range(100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_after_detoxification, std_after_detoxification = evaluate_toxicity(model=ppo_model, \n",
    "                                                                        toxicity_evaluator=toxicity_evaluator, \n",
    "                                                                        tokenizer=tokenizer, \n",
    "                                                                        dataset=dataset[\"test\"], \n",
    "                                                                        num_samples=10)\n",
    "print(f'toxicity [mean, std] after detox: [{mean_after_detoxification}, {std_after_detoxification}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_improvement = (mean_before_detoxification - mean_after_detoxification) / mean_before_detoxification\n",
    "std_improvement = (std_before_detoxification - std_after_detoxification) / std_before_detoxification\n",
    "\n",
    "print(f'Percentage improvement of toxicity score after detoxification:')\n",
    "print(f'mean: {mean_improvement*100:.2f}%')\n",
    "print(f'std: {std_improvement*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "compare_results = {}\n",
    "\n",
    "df_batch = dataset[\"test\"][0:batch_size]\n",
    "\n",
    "compare_results[\"query\"] = df_batch[\"query\"]\n",
    "prompt_tensors = df_batch[\"input_ids\"]\n",
    "\n",
    "summary_tensors_ref = []\n",
    "summary_tensors = []\n",
    "\n",
    "# Get response from ppo and base model.\n",
    "for i in tqdm(range(batch_size)):\n",
    "    gen_len = output_length_sampler()\n",
    "    generation_kwargs[\"max_new_tokens\"] = gen_len\n",
    "    \n",
    "    summary = ref_model.generate(\n",
    "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device), \n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors_ref.append(summary)\n",
    "\n",
    "    summary = ppo_model.generate(\n",
    "        input_ids=torch.as_tensor(prompt_tensors[i]).unsqueeze(dim=0).to(device), \n",
    "        **generation_kwargs\n",
    "    ).squeeze()[-gen_len:]\n",
    "    summary_tensors.append(summary)\n",
    "\n",
    "# Decode responses.\n",
    "compare_results[\"response_before\"] = [tokenizer.decode(summary_tensors_ref[i]) for i in range(batch_size)]\n",
    "compare_results[\"response_after\"] = [tokenizer.decode(summary_tensors[i]) for i in range(batch_size)]\n",
    "\n",
    "# Sentiment analysis of query/response pairs before/after.\n",
    "texts_before = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_before\"])]\n",
    "rewards_before = sentiment_pipe(texts_before, **reward_kwargs)\n",
    "compare_results[\"reward_before\"] = [reward[not_hate_index][\"score\"] for reward in rewards_before]\n",
    "\n",
    "texts_after = [d + s for d, s in zip(compare_results[\"query\"], compare_results[\"response_after\"])]\n",
    "rewards_after = sentiment_pipe(texts_after, **reward_kwargs)\n",
    "compare_results[\"reward_after\"] = [reward[not_hate_index][\"score\"] for reward in rewards_after]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', 500)\n",
    "df_compare_results = pd.DataFrame(compare_results)\n",
    "df_compare_results[\"reward_diff\"] = df_compare_results['reward_after'] - df_compare_results['reward_before']\n",
    "df_compare_results_sorted = df_compare_results.sort_values(by=['reward_diff'], ascending=False).reset_index(drop=True)\n",
    "df_compare_results_sorted"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
